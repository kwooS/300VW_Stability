{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23067,"status":"ok","timestamp":1660205884163,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"hcbT16kWQ8y3","outputId":"f22e432c-90f9-41d7-991a-58abe0972723"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","import numpy as np\n","import os\n","import cv2\n","import pandas as pd\n","import tensorflow as tf\n","import torch.nn as nn\n","import torch\n","import itertools \n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660205884164,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"VIDf3pgXQ_Lj"},"outputs":[],"source":["#load \"num\" numbers of frame and annot data per video. Insufficient RAM for full frame and annot data \n","#int num gives num amount of frames, float num gives that much percentage of frames\n","def load_data(dir, num):\n","  img_data = []\n","  annot_data = []\n","  for i in sorted(os.listdir(dir)):\n","    f = os.path.join(dir, i) # ...training/001 etc\n","    if not os.path.isdir(f):\n","      continue\n","    total_num = len(os.listdir(f+\"/frames\"))\n","    if total_num ==0:\n","      continue\n","    if isinstance(num, int):\n","      chosen_start = np.random.choice(total_num-num, 1, replace=False)\n","    elif isinstance(num, float):\n","      chosen_idx = np.random.choice(total_num, int(total_num*num), replace=False)\n","    else:\n","      print(\"invalid num\")\n","      return 0, 0\n","\n","    chosen_idx = []\n","    for x in range(num):\n","      chosen_idx.append(chosen_start[0] + x)\n","\n","    frame_list = sorted(os.listdir(f+\"/frames\"), key=lambda x: int(x[5:-4]))\n","    annot_list = sorted(os.listdir(f+\"/annot\"), key=lambda x: int(x[:6]))\n","\n","    for j in chosen_idx:  \n","      frame_num = frame_list[j] #iterate over frames file for img\n","      img_path = os.path.join(f+\"/frames\",frame_num)\n","      image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","      if image.shape[0] != 720:\n","        continue\n","      img_data.append(image.T)\n","\n","      annot_num = annot_list[j] #iterate over annot file for coords\n","      annot_path = os.path.join(f+\"/annot\",annot_num)\n","      annot = np.loadtxt(annot_path, comments=(\"version:\", \"n_points:\", \"{\", \"}\"))\n","\n","      annot_data.append(annot.flatten())\n","  return img_data, annot_data"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2608575,"status":"ok","timestamp":1660208492735,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"3IkaiumpRCbS"},"outputs":[],"source":["root = \"/content/gdrive/My Drive/ESTsoft Internship/300VW_Dataset_2015_12_14/training\"\n","#root = \"/content/gdrive/Shared with me/ESTsoft Internship/300VW_Dataset_2015_12_14/training\"\n","\n","continuous = 20 #number of continuous frames\n","X_train, y_train = load_data(root, continuous)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660208492735,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"gx9uG964RITF"},"outputs":[],"source":["class baseline_model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # (3, 1280, 720)\n","    # output size = int((input_size + 2*padding - filter)/stride) + 1\n","    # number of filter = input_channel\n","    layers = [\n","        nn.Conv2d(3, 16, 5, padding=2),  # (16,1280,720)\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),         \n","        nn.MaxPool2d((4,3)),  # (16, 320, 240)\n","        nn.Dropout2d(),\n","\n","        nn.Conv2d(16, 32, 5, padding = 2), # (32, 320, 240)\n","        nn.ReLU(),\n","        nn.Conv2d(32, 32, 3, padding=1),        \n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),        \n","        nn.MaxPool2d((4,3)), # (32, 80, 80)\n","        nn.Dropout(),\n","        \n","        nn.Conv2d(32, 64, 3, padding = 1),  # (64, 80, 80)\n","        nn.ReLU(),\n","        nn.Conv2d(64, 64, 3, padding=1),        \n","        nn.BatchNorm2d(64),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),    # (64, 40, 40)  \n","        nn.Dropout(),\n","        \n","        nn.Conv2d(64, 128, 3, padding = 1),  # (128, 40, 40)\n","        nn.ReLU(),\n","        nn.Conv2d(128, 128, 3, padding=1),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),    # (128, 20, 20)\n","        nn.Dropout(),\n","\n","        nn.Conv2d(128, 256, 3, padding = 1),  # (256, 20, 20)\n","        nn.ReLU(),\n","        nn.Conv2d(256, 256, 3, padding=1),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),   # (256, 10, 10)\n","        nn.Dropout(),\n","\n","        nn.Conv2d(256, 512, 3, padding = 1),  # (512, 10, 10)\n","        nn.ReLU(),\n","        nn.Conv2d(512, 512, 3, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.ReLU(),\n","        nn.Dropout(),\n","        nn.MaxPool2d(2)  # (512, 5, 5)\n","    ]\n","\n","    self.layers = nn.ModuleList(layers)\n","    self.linear1 = nn.Linear(512*5*5, 512) \n","    self.relu = nn.ReLU()\n","    self.linear2 = nn.Linear(512, 136)\n","\n","  def forward(self, x):\n","    temp = None\n","    h = x\n","    for i in range(len(self.layers)):\n","      if (i % 7 == 0 and i!=0):\n","        temp = h\n","        h = self.layers[i](h) + temp\n","      else :\n","        h = self.layers[i](h)\n","    h = self.linear1(h.reshape(-1,512*25))\n","    h = self.relu(h)\n","    h = self.linear2(h)\n","    return h"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4001,"status":"ok","timestamp":1660208496733,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"kWb4H_fX2-DY"},"outputs":[],"source":["model = baseline_model()\n","model_new_standard = baseline_model()\n","\n","model = model.cuda()\n","model_new_standard = model_new_standard.cuda()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660208496733,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"zBCyhW4GRM3N"},"outputs":[],"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","optimizer_n = torch.optim.Adam(model_new_standard.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100788,"status":"ok","timestamp":1660208597519,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"6hMC_bmZRYFr","outputId":"343bafde-437c-40c0-e876-31490f8ccda8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  import sys\n"]}],"source":["#data\n","X_train_np = np.stack(X_train)\n","y_train_np = np.stack(y_train)\n","\n","X_train_np = torch.tensor(X_train_np)\n","y_train_np = torch.tensor(y_train_np)\n","\n","train_dataset = TensorDataset(X_train_np, y_train_np)\n","train_dataloader = DataLoader(train_dataset)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3249121,"status":"ok","timestamp":1660211846636,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"c5kzgkzaSE1O","outputId":"63a32cdc-26b0-492d-b4d4-8548e4507ff0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 Epoch loss: 36268.34307731346\n","1 Epoch loss: 6193.227738655938\n","2 Epoch loss: 6293.201673493562\n","3 Epoch loss: 5175.835874387953\n","4 Epoch loss: 4186.23269353796\n","5 Epoch loss: 2843.294541277709\n","6 Epoch loss: 2199.706755927757\n","7 Epoch loss: 1630.4369399494594\n","8 Epoch loss: 1510.6089583290948\n","9 Epoch loss: 1596.7797979213574\n","10 Epoch loss: 1449.8926023094743\n","11 Epoch loss: 1307.0319834921095\n","12 Epoch loss: 1173.544247037393\n","13 Epoch loss: 1296.734477382236\n","14 Epoch loss: 1018.0336188563595\n","15 Epoch loss: 1017.7640664524503\n","16 Epoch loss: 972.6858133492647\n","17 Epoch loss: 964.902910455068\n","18 Epoch loss: 995.0148858882762\n","19 Epoch loss: 937.6086647457547\n","20 Epoch loss: 883.4346124931618\n","21 Epoch loss: 923.908103522548\n","22 Epoch loss: 947.50479893861\n","23 Epoch loss: 906.4070312288072\n","24 Epoch loss: 967.5946617797568\n","25 Epoch loss: 909.4795393378646\n","26 Epoch loss: 995.8659631464217\n","27 Epoch loss: 873.8276263272321\n","28 Epoch loss: 843.5653863500665\n","29 Epoch loss: 839.030314733364\n","30 Epoch loss: 796.0755515027929\n","31 Epoch loss: 764.985197268592\n","32 Epoch loss: 765.7714358038372\n","33 Epoch loss: 791.2989246518524\n","34 Epoch loss: 729.1642167020727\n","35 Epoch loss: 638.3868607220826\n","36 Epoch loss: 584.6536531801577\n","37 Epoch loss: 611.0553301440345\n","38 Epoch loss: 612.0251174079048\n","39 Epoch loss: 582.3627725777802\n","40 Epoch loss: 521.161430949635\n","41 Epoch loss: 497.00662113295664\n","42 Epoch loss: 485.88064433380407\n","43 Epoch loss: 467.2587051612359\n","44 Epoch loss: 449.6329396000615\n","45 Epoch loss: 417.1654534189789\n","46 Epoch loss: 476.6389798844302\n","47 Epoch loss: 403.5191279605583\n","48 Epoch loss: 371.34992628185836\n","49 Epoch loss: 369.84320149421694\n","50 Epoch loss: 351.6391872918164\n","51 Epoch loss: 345.30086706391086\n","52 Epoch loss: 371.34800183596434\n","53 Epoch loss: 376.1550595407133\n","54 Epoch loss: 380.8654942397718\n","55 Epoch loss: 390.7588224163762\n","56 Epoch loss: 385.1294809710096\n","57 Epoch loss: 400.7565187666151\n","58 Epoch loss: 422.9384813096788\n","59 Epoch loss: 340.29807346926793\n","60 Epoch loss: 337.6970109436247\n","61 Epoch loss: 316.40292935724614\n","62 Epoch loss: 345.47657770580713\n","63 Epoch loss: 305.7543346616957\n","64 Epoch loss: 282.12357556731615\n","65 Epoch loss: 247.61781169485164\n","66 Epoch loss: 239.4785642200046\n","67 Epoch loss: 271.3591519753138\n","68 Epoch loss: 278.3751531512649\n","69 Epoch loss: 253.17975647802706\n","70 Epoch loss: 277.9859418401012\n","71 Epoch loss: 252.35659055091716\n","72 Epoch loss: 270.211139939891\n","73 Epoch loss: 250.4334993697979\n","74 Epoch loss: 244.08618884616428\n","75 Epoch loss: 235.09623351891835\n","76 Epoch loss: 254.72671298980714\n","77 Epoch loss: 230.54482024113338\n","78 Epoch loss: 234.02600983160514\n","79 Epoch loss: 252.85011835628086\n","80 Epoch loss: 250.08913156456418\n","81 Epoch loss: 263.29374094804126\n","82 Epoch loss: 253.86553513385633\n","83 Epoch loss: 262.55670494812506\n","84 Epoch loss: 312.5629062025635\n","85 Epoch loss: 314.54480620843395\n","86 Epoch loss: 267.755827526693\n","87 Epoch loss: 210.59045673652932\n","88 Epoch loss: 197.00244243674808\n","89 Epoch loss: 177.06770613237663\n","90 Epoch loss: 215.45799401159638\n","91 Epoch loss: 205.1968822033317\n","92 Epoch loss: 213.62957887296324\n","93 Epoch loss: 225.1423713101281\n","94 Epoch loss: 188.44286147135276\n","95 Epoch loss: 210.20981979811634\n","96 Epoch loss: 218.5301263888677\n","97 Epoch loss: 192.39716159855877\n","98 Epoch loss: 203.77003738262036\n","99 Epoch loss: 185.85587830940884\n","100 Epoch loss: 203.4336679140727\n","101 Epoch loss: 200.8718388583925\n","102 Epoch loss: 198.47088758327342\n","103 Epoch loss: 203.95091123051114\n","104 Epoch loss: 162.0102164224342\n","105 Epoch loss: 137.0932600869073\n","106 Epoch loss: 188.26342563143484\n","107 Epoch loss: 182.28169177770616\n","108 Epoch loss: 190.18971071949713\n","109 Epoch loss: 176.34123647477892\n","110 Epoch loss: 167.39892870408517\n","111 Epoch loss: 230.13401790018435\n","112 Epoch loss: 157.97094402092475\n","113 Epoch loss: 188.4763819862295\n","114 Epoch loss: 168.73000857211926\n","115 Epoch loss: 196.36435078691554\n","116 Epoch loss: 214.74705220328437\n","117 Epoch loss: 218.20394572328638\n","118 Epoch loss: 196.81223474873437\n","119 Epoch loss: 179.6411520379561\n","120 Epoch loss: 178.25249052400943\n","121 Epoch loss: 160.33530347258957\n","122 Epoch loss: 148.2621255883464\n","123 Epoch loss: 162.85661629924067\n","124 Epoch loss: 163.01187494595845\n","125 Epoch loss: 148.82023661401536\n","126 Epoch loss: 158.60204847256344\n","127 Epoch loss: 154.31068039426097\n","128 Epoch loss: 176.3343194935057\n","129 Epoch loss: 143.23781056359962\n","130 Epoch loss: 151.7804478406906\n","131 Epoch loss: 146.09688801279773\n","132 Epoch loss: 155.90673341132975\n","133 Epoch loss: 171.64336584983047\n","134 Epoch loss: 164.63995170107594\n","135 Epoch loss: 185.42619061425881\n","136 Epoch loss: 139.94267444522293\n","137 Epoch loss: 120.40043181423788\n","138 Epoch loss: 123.41173162901843\n","139 Epoch loss: 116.47061327386785\n","140 Epoch loss: 116.95763166259836\n","141 Epoch loss: 99.18061123424106\n","142 Epoch loss: 119.09007636529428\n","143 Epoch loss: 102.027412420732\n","144 Epoch loss: 110.62492388619317\n","145 Epoch loss: 105.77215179514002\n","146 Epoch loss: 106.7810387024173\n","147 Epoch loss: 120.38918193667023\n","148 Epoch loss: 132.6925067919272\n","149 Epoch loss: 108.28134442611977\n","150 Epoch loss: 104.47173768414392\n","151 Epoch loss: 115.98567383863308\n","152 Epoch loss: 130.29800995632453\n","153 Epoch loss: 111.42973112397723\n","154 Epoch loss: 133.37737284766303\n","155 Epoch loss: 126.89509091774623\n","156 Epoch loss: 128.68244555217248\n","157 Epoch loss: 131.21842853846374\n","158 Epoch loss: 129.61969472390635\n","159 Epoch loss: 109.48980961464069\n","160 Epoch loss: 98.9207023549963\n","161 Epoch loss: 98.2147507749222\n","162 Epoch loss: 108.49963011388425\n","163 Epoch loss: 111.57241899216616\n","164 Epoch loss: 101.0350798037317\n","165 Epoch loss: 101.93312264548408\n","166 Epoch loss: 101.89911405532449\n","167 Epoch loss: 96.45696873885613\n","168 Epoch loss: 104.34381597969266\n","169 Epoch loss: 108.52387872426598\n","170 Epoch loss: 120.64322653059607\n","171 Epoch loss: 161.1126399450832\n","172 Epoch loss: 151.78984360739037\n","173 Epoch loss: 149.4278505175202\n","174 Epoch loss: 115.52429145088902\n","175 Epoch loss: 113.58428722443404\n","176 Epoch loss: 98.67745889866794\n","177 Epoch loss: 113.48312989009752\n","178 Epoch loss: 128.7250592885194\n","179 Epoch loss: 125.41415312312267\n","180 Epoch loss: 116.54287593408867\n","181 Epoch loss: 99.47491272687913\n","182 Epoch loss: 96.8712107177134\n","183 Epoch loss: 115.8604423628913\n","184 Epoch loss: 98.818521870176\n","185 Epoch loss: 108.40973360119042\n","186 Epoch loss: 93.67631509789715\n","187 Epoch loss: 103.38919907190181\n","188 Epoch loss: 115.55302032795217\n","189 Epoch loss: 132.45641549781516\n","190 Epoch loss: 100.08234173059464\n","191 Epoch loss: 97.03996796431365\n","192 Epoch loss: 85.07457058319339\n","193 Epoch loss: 91.30055846660225\n","194 Epoch loss: 107.23015652718368\n","195 Epoch loss: 108.53615979684724\n","196 Epoch loss: 109.07113648123212\n","197 Epoch loss: 88.34820878373252\n","198 Epoch loss: 92.4337045263361\n","199 Epoch loss: 100.98190804675774\n"]}],"source":["model.train()\n","\n","num_epochs = 200\n","for epoch in range(num_epochs):\n","  batch_loss_c = []\n","  for batch_idx, (inputs, target) in enumerate(train_dataloader):\n","  #for inputs, target in zip(X_train_np, y_train_np):\n","    inputs = inputs.float()\n","    inputs = inputs.cuda()\n","    target = target.type(torch.FloatTensor)\n","    target = target.cuda()\n","    out = model(inputs)\n","    loss = criterion(out, target)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    batch_loss_c.append(loss.item())\n","  print(epoch, \"Epoch loss: \" + str(sum(batch_loss_c)/len(batch_loss_c)))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1660211846637,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"X9q6WTEGTWcS"},"outputs":[],"source":["torch.save(model.state_dict(), \"/content/gdrive/Shared with me/ESTsoft Internship/model.pt\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1660211846637,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"SiWostucYKTm"},"outputs":[],"source":["def indicator(curr, prev):\n","  if prev == None:\n","    data_for_next_step = []\n","    for i in curr:\n","      center_curr_x = (i[54]+i[66]+i[72]+i[90])/4\n","      center_curr_y = (i[55]+i[67]+i[73]+i[91])/4\n","      data_per_frame = []\n","      for k in range(68):\n","        dist_curr = ((i[2*k]-center_curr_x)**2 + (i[2*k+1]-center_curr_y)**2 + 1)**0.5          \n","        data_per_frame.append(dist_curr)\n","      data_for_next_step.append(data_per_frame)\n","    return data_for_next_step, 0\n","  \n","  total_ind = []\n","  data_for_next_step = []\n","  for i in range(len(curr)):\n","    center_curr_x = (curr[i][54]+curr[i][66]+curr[i][72]+curr[i][90])/4\n","    center_curr_y = (curr[i][55]+curr[i][67]+curr[i][73]+curr[i][91])/4\n","\n","    point_ind = []\n","    data_per_frame = []\n","    for j in range(68):\n","      dist_curr = ((curr[i][2*j]-center_curr_x)**2 + (curr[i][2*j+1]-center_curr_y)**2 + 1)**0.5\n","      point_ind.append(dist_curr/prev[i][j])\n","      data_per_frame.append(dist_curr)\n","    total_ind.append(max(point_ind))\n","    data_for_next_step.append(data_per_frame)\n","  return data_for_next_step, abs(max(total_ind) - 1)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3474330,"status":"ok","timestamp":1660215320956,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"NF77RbB3YLcf","outputId":"eee63520-29ac-4155-c091-fde99d0656b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 Epoch loss: 35745.01816233882\n","1 Epoch loss: 6181.009503272728\n","2 Epoch loss: 5496.811677212186\n","3 Epoch loss: 5400.294537233423\n","4 Epoch loss: 4279.65853156337\n","5 Epoch loss: 3375.2322134865653\n","6 Epoch loss: 2503.4339504524514\n","7 Epoch loss: 2065.5625474788526\n","8 Epoch loss: 1704.944045582524\n","9 Epoch loss: 1481.9799641997727\n","10 Epoch loss: 1343.57186877286\n","11 Epoch loss: 1109.2784786471614\n","12 Epoch loss: 1252.2326113877473\n","13 Epoch loss: 1194.7757765169497\n","14 Epoch loss: 1079.947544059047\n","15 Epoch loss: 1036.402349585074\n","16 Epoch loss: 1066.064029983238\n","17 Epoch loss: 968.4496233516269\n","18 Epoch loss: 1050.8196352605466\n","19 Epoch loss: 928.1259347880329\n","20 Epoch loss: 864.5313400445161\n","21 Epoch loss: 927.432935723552\n","22 Epoch loss: 976.4013687504662\n","23 Epoch loss: 889.3174206945631\n","24 Epoch loss: 856.0624875174628\n","25 Epoch loss: 822.0733832500599\n","26 Epoch loss: 775.9120901390359\n","27 Epoch loss: 762.0341835701907\n","28 Epoch loss: 741.8618522873631\n","29 Epoch loss: 735.5620282402745\n","30 Epoch loss: 718.9609776231978\n","31 Epoch loss: 694.2733531307291\n","32 Epoch loss: 673.1739923998161\n","33 Epoch loss: 629.923644122371\n","34 Epoch loss: 629.9124620605398\n","35 Epoch loss: 682.8571900897556\n","36 Epoch loss: 561.0530520085936\n","37 Epoch loss: 541.4887853887346\n","38 Epoch loss: 590.1853900194168\n","39 Epoch loss: 579.4585980327041\n","40 Epoch loss: 474.98248350178756\n","41 Epoch loss: 507.9678698107048\n","42 Epoch loss: 504.7634251523901\n","43 Epoch loss: 437.72943542003634\n","44 Epoch loss: 406.6507529488316\n","45 Epoch loss: 355.1144229685819\n","46 Epoch loss: 389.5700374099943\n","47 Epoch loss: 350.87963895091303\n","48 Epoch loss: 355.76934956179724\n","49 Epoch loss: 389.1410014329133\n","50 Epoch loss: 383.16068177753027\n","51 Epoch loss: 349.2110593310109\n","52 Epoch loss: 400.4741295222883\n","53 Epoch loss: 415.11749936562995\n","54 Epoch loss: 388.0531412001009\n","55 Epoch loss: 377.49287549654645\n","56 Epoch loss: 376.5691949535299\n","57 Epoch loss: 344.5137865693481\n","58 Epoch loss: 355.0546361958539\n","59 Epoch loss: 345.37869463496736\n","60 Epoch loss: 308.41704135647524\n","61 Epoch loss: 291.600147986412\n","62 Epoch loss: 261.4154737675631\n","63 Epoch loss: 244.74253332349988\n","64 Epoch loss: 296.87082920869193\n","65 Epoch loss: 225.94554339161627\n","66 Epoch loss: 231.12105082405938\n","67 Epoch loss: 223.51036408389055\n","68 Epoch loss: 248.5529042085012\n","69 Epoch loss: 208.2346007929908\n","70 Epoch loss: 220.41044041668928\n","71 Epoch loss: 246.2374260549192\n","72 Epoch loss: 280.6323077157692\n","73 Epoch loss: 258.26863600677916\n","74 Epoch loss: 290.4620015568203\n","75 Epoch loss: 252.4643858750661\n","76 Epoch loss: 209.00465864870284\n","77 Epoch loss: 261.14557060135735\n","78 Epoch loss: 223.55574594956857\n","79 Epoch loss: 216.27108228294938\n","80 Epoch loss: 207.86609107476693\n","81 Epoch loss: 188.04352000024585\n","82 Epoch loss: 199.6994142382233\n","83 Epoch loss: 220.7292837796388\n","84 Epoch loss: 188.72272966879385\n","85 Epoch loss: 221.6727456777184\n","86 Epoch loss: 208.2665861165082\n","87 Epoch loss: 190.1184348909943\n","88 Epoch loss: 174.43812568452623\n","89 Epoch loss: 181.64265633159215\n","90 Epoch loss: 203.80257437449913\n","91 Epoch loss: 209.59014518481715\n","92 Epoch loss: 195.25607243643867\n","93 Epoch loss: 188.60922976246587\n","94 Epoch loss: 175.65709392980293\n","95 Epoch loss: 205.4644076903661\n","96 Epoch loss: 177.71983328439572\n","97 Epoch loss: 170.91993459551423\n","98 Epoch loss: 152.99170567901047\n","99 Epoch loss: 190.53546548640287\n","100 Epoch loss: 191.79995409206109\n","101 Epoch loss: 152.11073191739894\n","102 Epoch loss: 161.86725915626243\n","103 Epoch loss: 170.5223201663406\n","104 Epoch loss: 175.36495364065524\n","105 Epoch loss: 228.11550829366402\n","106 Epoch loss: 233.84305415418413\n","107 Epoch loss: 177.28770058508272\n","108 Epoch loss: 182.91374308621442\n","109 Epoch loss: 184.9585337833122\n","110 Epoch loss: 189.7814906614798\n","111 Epoch loss: 178.30550885906925\n","112 Epoch loss: 171.94187946672793\n","113 Epoch loss: 163.53410863346522\n","114 Epoch loss: 166.72942215954816\n","115 Epoch loss: 157.2556826962365\n","116 Epoch loss: 159.80996876513515\n","117 Epoch loss: 138.61426139628446\n","118 Epoch loss: 151.8073483078568\n","119 Epoch loss: 145.95433064699174\n","120 Epoch loss: 152.65614383044067\n","121 Epoch loss: 144.83831434956303\n","122 Epoch loss: 140.05841435767985\n","123 Epoch loss: 149.1340913993341\n","124 Epoch loss: 152.13435877252508\n","125 Epoch loss: 141.4464573268537\n","126 Epoch loss: 155.73443026366058\n","127 Epoch loss: 180.99845436678993\n","128 Epoch loss: 194.49291906974935\n","129 Epoch loss: 156.36205921438005\n","130 Epoch loss: 146.67119242173655\n","131 Epoch loss: 128.76458534487972\n","132 Epoch loss: 137.58727797932096\n","133 Epoch loss: 150.33262493301322\n","134 Epoch loss: 156.4307052983178\n","135 Epoch loss: 127.6887083530426\n","136 Epoch loss: 142.05423040345863\n","137 Epoch loss: 123.88613988470148\n","138 Epoch loss: 117.40964845507233\n","139 Epoch loss: 140.25235029370697\n","140 Epoch loss: 143.75904155086587\n","141 Epoch loss: 130.8870787832472\n","142 Epoch loss: 110.51397929898015\n","143 Epoch loss: 113.25048921373156\n","144 Epoch loss: 113.77123867140875\n","145 Epoch loss: 144.82538089752197\n","146 Epoch loss: 131.44586387961\n","147 Epoch loss: 145.2007281029666\n","148 Epoch loss: 142.46741437205563\n","149 Epoch loss: 126.07820568923597\n","150 Epoch loss: 104.66184190070187\n","151 Epoch loss: 113.09314706413834\n","152 Epoch loss: 139.52983959604194\n","153 Epoch loss: 191.31899197763866\n","154 Epoch loss: 143.63708314763176\n","155 Epoch loss: 111.1176853745072\n","156 Epoch loss: 95.00725621735609\n","157 Epoch loss: 91.25678300416028\n","158 Epoch loss: 107.36830082778577\n","159 Epoch loss: 100.94069895479414\n","160 Epoch loss: 103.12510738726016\n","161 Epoch loss: 104.21704645068557\n","162 Epoch loss: 93.4022702049326\n","163 Epoch loss: 100.11811539464527\n","164 Epoch loss: 92.14004717402987\n","165 Epoch loss: 99.70087076955372\n","166 Epoch loss: 100.33697561378833\n","167 Epoch loss: 86.5809152159426\n","168 Epoch loss: 100.64055986271964\n","169 Epoch loss: 124.24925304761639\n","170 Epoch loss: 115.45624470512072\n","171 Epoch loss: 116.70989465978411\n","172 Epoch loss: 97.38964082709064\n","173 Epoch loss: 104.65093486309051\n","174 Epoch loss: 102.14299673592603\n","175 Epoch loss: 94.73009574060087\n","176 Epoch loss: 92.92223213712374\n","177 Epoch loss: 93.0044631185355\n","178 Epoch loss: 106.90424549358862\n","179 Epoch loss: 111.17975949932028\n","180 Epoch loss: 109.18504473500782\n","181 Epoch loss: 104.44084026769356\n","182 Epoch loss: 96.75743010176552\n","183 Epoch loss: 107.75529245049866\n","184 Epoch loss: 101.40872009860145\n","185 Epoch loss: 91.12348635108383\n","186 Epoch loss: 101.51270607400824\n","187 Epoch loss: 99.19525372496358\n","188 Epoch loss: 97.30161460152378\n","189 Epoch loss: 98.05506602304953\n","190 Epoch loss: 112.51540398818476\n","191 Epoch loss: 117.35602449487757\n","192 Epoch loss: 113.93036272768622\n","193 Epoch loss: 101.27262895018966\n","194 Epoch loss: 93.31165861465313\n","195 Epoch loss: 98.07441765092037\n","196 Epoch loss: 110.76551714782362\n","197 Epoch loss: 98.83654982867064\n","198 Epoch loss: 104.09001537208204\n","199 Epoch loss: 93.4665808684296\n"]}],"source":["model_new_standard.train()\n","\n","num_epochs = 200\n","for epoch in range(num_epochs):\n","  batch_loss = []\n","  prev_out_calcs = None\n","  count = 0\n","  prev_loss = 100\n","  for batch_idx, (inputs, target) in enumerate(train_dataloader):\n","  #for inputs, target in zip(X_train_np, y_train_np):  \n","    inputs = inputs.float()\n","    inputs = inputs.cuda()\n","    target = target.type(torch.FloatTensor)\n","    target = target.cuda()\n","    out = model_new_standard(inputs)\n","    if prev_loss < 100:\n","      if count % continuous == 0:\n","        prev_out_calcs = None\n","      result = out.clone().detach()\n","      prev_out_calcs, indicator_result = indicator(result, prev_out_calcs)\n","      loss = criterion(out, target) + indicator_result #mse + indicator\n","    else:\n","      loss = criterion(out, target)\n","    prev_loss = loss.item()\n","    optimizer_n.zero_grad()\n","    loss.backward()\n","    optimizer_n.step()\n","    batch_loss.append(loss.item())\n","    count += 1\n","  print(epoch, \"Epoch loss: \" + str(sum(batch_loss)/len(batch_loss)))"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1243,"status":"ok","timestamp":1660215322195,"user":{"displayName":"Kun-Woo Song","userId":"10698102556996821434"},"user_tz":-540},"id":"JeoahXkfaCfp"},"outputs":[],"source":["torch.save(model_new_standard.state_dict(), \"/content/gdrive/Shared with me/ESTsoft Internship/model_new_standard.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Model Training","provenance":[{"file_id":"1q5GTsfPyBosjpSrOiORVdZcwXqmErIM_","timestamp":1660007746014}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
